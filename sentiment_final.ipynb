{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKXrr9yT-1RY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0m-hZnpBojM",
        "outputId": "2a352de4-ab71-4f88-d4cf-f493957774c4"
      },
      "outputs": [],
      "source": [
        "# Download resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "XcWXKBPuG_lU",
        "outputId": "1087bc79-bec1-468f-b9c1-d1aeb50c00ae"
      },
      "outputs": [],
      "source": [
        "reviews_df = pd.read_csv('/content/clothing_reviews.csv')\n",
        "description_df = pd.read_csv('/content/clothing_description.csv')\n",
        "\n",
        "# Merge the data on 'product_id'\n",
        "df = pd.merge(\n",
        "    reviews_df,\n",
        "    description_df[['product_id', 'sub_category']],  # Select only the columns needed\n",
        "    on='product_id',\n",
        "    how='left'  # Keep all rows from reviews_df even if no match in description_df\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNGhhCgwB8is"
      },
      "outputs": [],
      "source": [
        "# Drop missing reviews\n",
        "df.dropna(subset=['Review Text'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86s5JOlOCJiA"
      },
      "outputs": [],
      "source": [
        "# Filter to exclude only ratings 1-5 (in case of outliers)\n",
        "df = df[df['Rating'].isin([1, 2, 3, 4, 5])]\n",
        "\n",
        "# Re-map sentiment based on new condition:\n",
        "# Rating 1 or 2 → 0 (negative), 3 to 5 → 1 (positive)\n",
        "df['sentiment'] = df['Rating'].apply(lambda x: 1 if x >= 3 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmdbH-ThCmP9"
      },
      "outputs": [],
      "source": [
        "# Clean review text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation/numbers\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_review'] = df['Review Text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A79byIiKCppj"
      },
      "outputs": [],
      "source": [
        "# Lemmatize and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    words = text.split()\n",
        "    filtered = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "df['final_review'] = df['cleaned_review'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugGnZjjXEgvA"
      },
      "outputs": [],
      "source": [
        "# Split into training and test sets (before vectorization)\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    df['final_review'], df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment']\n",
        ")\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "\n",
        "# Fit only on training data\n",
        "xv_train = tfidf.fit_transform(X_train_text)\n",
        "\n",
        "# Transform test data\n",
        "xv_test = tfidf.transform(X_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY8Fc55ZZZpc",
        "outputId": "10582178-cfc7-4270-c33b-c54fb4b93265"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "y_train_original = y_train.copy()\n",
        "\n",
        "# Before SMOTE\n",
        "print(\"Class distribution before SMOTE:\")\n",
        "print(Counter(y_train_original))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBefOczEZupS",
        "outputId": "b8108480-2aa3-441f-8025-c15774caff6b"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "xv_train, y_train = smote.fit_resample(xv_train, y_train)\n",
        "\n",
        "# After SMOTE\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(Counter(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35U9fUYuC_EA"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLd8mMPnEoQ5"
      },
      "outputs": [],
      "source": [
        "svc = SVC()\n",
        "knc = KNeighborsClassifier()\n",
        "mnb = MultinomialNB()\n",
        "dtc = DecisionTreeClassifier()\n",
        "lrc = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "abc = AdaBoostClassifier()\n",
        "bc = BaggingClassifier()\n",
        "etc = ExtraTreesClassifier()\n",
        "gbdt = GradientBoostingClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsqyL5wOErfo"
      },
      "outputs": [],
      "source": [
        "clfs = {\n",
        "    'SVC': svc,\n",
        "    'KN': knc,\n",
        "    'NB': mnb,\n",
        "    'DT': dtc,\n",
        "    'LR': lrc,\n",
        "    'RF': rfc,\n",
        "    'AdaBoost': abc,\n",
        "    'BgC': bc,\n",
        "    'ETC': etc,\n",
        "    'GBDT':gbdt\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmKEokfCHyDD",
        "outputId": "d5c635ab-bd5c-40a9-aec4-e893f1eb2f4f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "results = []\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "for name, clf in clfs.items():\n",
        "    print(f\"\\nTraining {name} ...\")\n",
        "    try:\n",
        "        clf.fit(xv_train, y_train)\n",
        "        y_pred = clf.predict(xv_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}\")\n",
        "        results.append((name, acc, prec))\n",
        "    except Exception as e:\n",
        "        print(f\"Error training {name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe6jFQ5iJEIF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnIbkD1fgBcp",
        "outputId": "907878da-a023-44cd-b990-a2349827c39a"
      },
      "outputs": [],
      "source": [
        "# Select top 3 models\n",
        "top_models = ['SVC', 'ETC', 'RF']\n",
        "\n",
        "for model_name in top_models:\n",
        "    clf = clfs[model_name]\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = clf.predict(xv_test)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, pos_label=1)\n",
        "    rec = recall_score(y_test, y_pred, pos_label=1)\n",
        "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "    specificity = tn / (tn + fp)\n",
        "    sensitivity = rec  # same as recall for positive class\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"Accuracy   : {acc:.4f}\")\n",
        "    print(f\"Precision  : {prec:.4f}\")\n",
        "    print(f\"Recall     : {rec:.4f}\")\n",
        "    print(f\"F1 Score   : {f1:.4f}\")\n",
        "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utxwb6XnBy-l",
        "outputId": "a0391925-a8df-478e-e467-94675a9093b9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Step 1: Train ETC on training data\n",
        "etc = ExtraTreesClassifier(random_state=42)\n",
        "etc.fit(xv_train, y_train)\n",
        "\n",
        "# Step 2: Predict sentiment for all reviews in the full dataset\n",
        "all_reviews_vectorized = tfidf.transform(df['final_review'])\n",
        "df['predicted_sentiment'] = etc.predict(all_reviews_vectorized)\n",
        "\n",
        "# Step 3: Group by sub_category and product to compute sentiment score (mean predicted sentiment)\n",
        "sentiment_scores = df.groupby(['sub_category', 'product_id', 'product_name'])['predicted_sentiment'].mean().reset_index()\n",
        "sentiment_scores.rename(columns={'predicted_sentiment': 'sentiment_score'}, inplace=True)\n",
        "\n",
        "# Step 4: Rank products within each sub_category\n",
        "sentiment_scores['rank_in_subcategory'] = sentiment_scores.groupby('sub_category')['sentiment_score'] \\\n",
        "                                                           .rank(method='dense', ascending=False)\n",
        "\n",
        "# Step 5: Get top 5 products per sub_category\n",
        "sentiment_scores = sentiment_scores.sort_values(['sub_category', 'rank_in_subcategory'])\n",
        "top5_per_subcategory = sentiment_scores.groupby('sub_category').head(5)\n",
        "\n",
        "# Step 6: Display the results\n",
        "print(top5_per_subcategory[['product_id','sub_category', 'product_name', 'sentiment_score', 'rank_in_subcategory']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN7LS73SN_ph",
        "outputId": "84bfa107-f46d-4bd5-94f3-04b7c358df35"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Train the Extra Trees Classifier\n",
        "etc = ExtraTreesClassifier(random_state=42)\n",
        "etc.fit(xv_train, y_train)\n",
        "\n",
        "# Predict sentiment for all reviews\n",
        "all_reviews_vectorized = tfidf.transform(df['final_review'])\n",
        "df['predicted_sentiment'] = etc.predict(all_reviews_vectorized)\n",
        "\n",
        "# Group by product_id and product_name to compute average sentiment score\n",
        "overall_scores = df.groupby(['product_id', 'product_name'])['predicted_sentiment'].mean().reset_index()\n",
        "overall_scores.rename(columns={'predicted_sentiment': 'sentiment_score'}, inplace=True)\n",
        "\n",
        "# Rank all products based on sentiment score\n",
        "overall_scores['overall_rank'] = overall_scores['sentiment_score'].rank(method='dense', ascending=False)\n",
        "\n",
        "# Sort and get top 10 products\n",
        "top_products = overall_scores.sort_values('sentiment_score', ascending=False).head(300)\n",
        "\n",
        "# Display result with product_id\n",
        "print(top_products[['product_id', 'product_name', 'sentiment_score', 'overall_rank']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7pWc5tpO5AQ"
      },
      "outputs": [],
      "source": [
        "top_products[['product_id', 'product_name', 'sentiment_score', 'overall_rank']].to_csv('top_ranked_products.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP0S17ySdbYE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
